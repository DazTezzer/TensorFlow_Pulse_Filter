# Компенсация шума в сигнале ЭМГ
Необходимо компенсировать шум в сигнале ЭМГ таким образом, чтобы уменьшить его составляющую в сигнале по возможности, не подавив изначальный сигнал.
Для обучения нейросети для этой задачи были выбраны TensorFlow и Keras.

## Keras
```
model = tf.keras.Sequential([
    tf.keras.layers.Conv1D(1, 41, 1, input_shape=(distorted_signal.shape[1], 1), padding='same')
])

model.compile(loss='mean_squared_error', optimizer=tf.keras.optimizers.SGD(learning_rate=0.00000000001)) # mean_squared_error - Эта функция оценивает среднеквадратичную разницу между ожидаемыми и предсказанными значениями.
history = model.fit(noise, distorted_signal, epochs=500, batch_size=64)
restored_signal = model.predict(distorted_signal)
return restored_signal , history

```
Здесь мы создаем sequential модель, используя tf.keras.Sequential. Модель состоит из одного сверточного слоя (tf.keras.layers.Conv1D).
Слой Conv1D применяет одномерную операцию свертки к входным данным. Он принимает следующие аргументы:
1: количество фильтров (выходных каналов) в сверточном слое.
41: Размер сверточного фильтра (размер ядра).
1: Шаг операции свертки.
input_shape=(distorted_signal.shape[1], 1): форма входных данных. distorted_signal.shape[1] представляет длину входного сигнала, а 1 представляет количество входных каналов.
padding='same': тип заполнения, применяемый к входным данным. «то же самое» дополнение означает, что выходные данные будут иметь те же пространственные размеры, что и входные.
Далее происходит компиляция модели:
model.compile(loss='mean_squared_error', optimizer=tf.keras.optimizers.SGD(learning_rate=0.00000000001))
Здесь мы настраиваем модель для обучения. Мы указываем функцию потерь (loss='mean_squared_error'), то есть MSE, которая вычисляет среднеквадратическую разницу между ожидаемыми (истинными) и прогнозируемыми значениями.

Мы также указываем оптимизатор, используя Optimizer=tf.keras.optimizers.SGD(learning_rate=0.00000000001). В данном случае мы используем стохастический градиентный спуск (SGD) с очень маленькой скоростью обучения (0,00000000001). Оптимизатор отвечает за обновление весов модели во время обучения, чтобы минимизировать потери.
Далее происходит обучение модели:
history = model.fit(noise, distorted_signal, epochs=500, batch_size=64)
Здесь мы обучаем модель, используя метод fit. Мы предоставляем noise в качестве входных данных и distorted_signal в качестве целевых (ожидаемых выходных) данных. Обучение выполняется в течение 500 эпох, а данные делятся на пакеты размером 64 для эффективных вычислений.
Метод fit обновляет веса модели на основе предоставленных данных и оптимизатора и возвращает объект истории, содержащий информацию о процессе обучения.
После обучения модели мы используем метод прогнозирования для получения прогнозируемого восстановленного сигнала на основе входного сигнала distorted_signal.
restored_signal = model.predict(distorted_signal)
В целом, мы создаем модель сверточной нейронной сети (CNN) с одним сверточным слоем. Модель обучается с использованием предоставленных входных и целевых данных, а прогнозируемый восстановленный сигнал возвращается вместе с историей обучения.
Таким образом, таким был график до обработки Keras:

![image](https://github.com/DazTezzer/TensorFlow_Pulse_Filter/assets/125472899/46ed4285-13ea-45b7-98d5-624690c2059b)


А таким он стал после обработки Keras:

![image](https://github.com/DazTezzer/TensorFlow_Pulse_Filter/assets/125472899/fc43060c-1dcd-45b6-ae3c-289f1427b027)


График функции потерь при обучении в Keras:

![image](https://github.com/DazTezzer/TensorFlow_Pulse_Filter/assets/125472899/9e32dfb8-78d9-41bf-a2f5-9977a666d79b)


## TensorFlow
Код для Tensorflow также реализует алгоритм адаптивной фильтрации для восстановления исходного сигнала из зашумленного с использованием TensorFlow и градиентного спуска.
Сначала необходимо указать параметры скорости обучения (learning rate) и памяти фильтра (M). 
Далее выполняются следующие шаги:
Подготовка данных: создаются матрицы и массивы для хранения сдвинутых версий входного сигнала (матрица X), коэффициентов фильтра (матрица C), выходных значений фильтра (массив y) и ошибок на каждой итерации (список errors).
Инициализация параметров: устанавливаются параметры фильтра, такие как скорость обучения (mu), память фильтра (M), и длина входного вектора (N).
Обучение адаптивного фильтра: используется TensorFlow для реализации градиентного спуска с целью минимизации квадратичной ошибки между ожидаемым и предсказанным выходом фильтра. Это выполняется с использованием структуры tf.function и tf.GradientTape.
Обновление коэффициентов фильтра: коэффициенты фильтра обновляются на каждой итерации с использованием градиентного спуска.
Сохранение результатов: предсказанные выходы, ошибки и изменения коэффициентов фильтра сохраняются для анализа и последующего использования.
Возвращение результатов: в конце процесса обучения возвращаются массив предсказанных выходов, список ошибок и список изменений коэффициентов фильтра.
Таким образом, обучение фильтра происходит с использованием метода градиентного спуска. Градиентный спуск — это оптимизационный алгоритм, который минимизирует функцию потерь, вычисляя градиент (производную) функции относительно параметров модели и обновляя параметры в направлении, противоположном градиенту. В данном случае градиент рассчитывается для функции потерь, которая представляет квадрат разницы между ожидаемым выходом фильтра и предсказанным выходом для каждой итерации. Обновление коэффициентов фильтра (переменной tC) происходит с использованием градиентного спуска в блоке tC.assign(tC - tf.math.scalar_mul(mu, grad)).


![image](https://github.com/DazTezzer/TensorFlow_Pulse_Filter/assets/125472899/211dad13-2ad5-4273-a91d-35929239ece5)


![image](https://github.com/DazTezzer/TensorFlow_Pulse_Filter/assets/125472899/f6c3d18e-4dc2-40d2-9adb-21bc77016535)


График функции потерь в Tensorflow (tensorflow_loss)

![image](https://github.com/DazTezzer/TensorFlow_Pulse_Filter/assets/125472899/7a3ad356-9a39-4bfe-9a28-6e99fd5dce99)


+ коэффициенты

![image](https://github.com/DazTezzer/TensorFlow_Pulse_Filter/assets/125472899/aa625b0d-b674-4cee-b10f-525d3d12b8e8)

